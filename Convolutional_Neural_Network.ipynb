{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Network.ipynb",
      "provenance": [],
      "mount_file_id": "1TWiCB3QJqsfe4NDR3qrryju4NxS_VQJr",
      "authorship_tag": "ABX9TyMdKIBhCkflXPOr46kaMOj0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukti845/Neural-Networks/blob/main/Convolutional_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlfgxHFQPWNq"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G36ARdZAPdIx"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTwroI2JPcP5"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3UFfLzW8Pt_p",
        "outputId": "9d4df0bc-cb50-4c7f-8305-0e85f4a05568"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFc18_WKP8Zr"
      },
      "source": [
        "## Part-1 Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5loCN17suGkO"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHgr4y_6P3Fo"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_x-KlidunHa",
        "outputId": "49ab03f0-d9a6-47b8-af14-3aeb8efbf9bb"
      },
      "source": [
        "training_set = train_datagen.flow_from_directory(\"/content/drive/MyDrive/cnn dataset/dataset/training_set\",\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6W2O6kHxhb8"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHhfmAYYxTUa",
        "outputId": "8fc1b1cb-bef3-4e7f-eb28-f28059da458c"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\"/content/drive/MyDrive/cnn dataset/dataset/test_set\",\n",
        "                        target_size = (64, 64),\n",
        "                        batch_size = 32,\n",
        "                        class_mode = 'binary')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVmVlXoPRn1v"
      },
      "source": [
        "## Part-2 Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDNWko4nRtt0"
      },
      "source": [
        "### Initializing the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXTeOAh-yHf-"
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRVpvhXMSUuw"
      },
      "source": [
        "### Step-1 Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bbJVkX8SPl6"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCWPAFTNWNt7"
      },
      "source": [
        "### Step-2 Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPwOiZZmTGGi"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_50cQmGzU_uO"
      },
      "source": [
        "#### Adding a second convolution layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiKSXPS1Tred"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb2oUbxtWFEw"
      },
      "source": [
        "### Step-3 Flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzP5tdkDWC6g"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoFyMjL2EUqY"
      },
      "source": [
        "### Step-4 Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJf3G1nWWlHH"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=120, activation='relu'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-9zOwWeFSSD"
      },
      "source": [
        "### Step-5 Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTh3safEFQUg"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu3rX8hKF0h1"
      },
      "source": [
        "## Part-3 Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-SlwGa4F7AP"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4JipjZiFcgb"
      },
      "source": [
        "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWEbhJuG_i8"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWVcXEz4GKga",
        "outputId": "a31998f1-0989-4733-bdb9-2190c634a801"
      },
      "source": [
        "cnn.fit(x = training_set, validation_data=test_set, epochs=25)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 1637s 7s/step - loss: 0.6795 - accuracy: 0.5649 - val_loss: 0.6225 - val_accuracy: 0.6685\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 80s 321ms/step - loss: 0.6074 - accuracy: 0.6711 - val_loss: 0.5654 - val_accuracy: 0.7115\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 80s 321ms/step - loss: 0.5692 - accuracy: 0.7041 - val_loss: 0.5620 - val_accuracy: 0.7200\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 80s 321ms/step - loss: 0.5245 - accuracy: 0.7439 - val_loss: 0.5372 - val_accuracy: 0.7415\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 80s 321ms/step - loss: 0.5013 - accuracy: 0.7554 - val_loss: 0.5040 - val_accuracy: 0.7550\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 80s 319ms/step - loss: 0.4832 - accuracy: 0.7641 - val_loss: 0.4873 - val_accuracy: 0.7760\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 79s 317ms/step - loss: 0.4716 - accuracy: 0.7782 - val_loss: 0.4879 - val_accuracy: 0.7720\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 81s 322ms/step - loss: 0.4577 - accuracy: 0.7818 - val_loss: 0.4946 - val_accuracy: 0.7710\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 80s 322ms/step - loss: 0.4398 - accuracy: 0.7946 - val_loss: 0.4736 - val_accuracy: 0.7820\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 80s 320ms/step - loss: 0.4293 - accuracy: 0.7986 - val_loss: 0.4812 - val_accuracy: 0.7870\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 80s 320ms/step - loss: 0.4142 - accuracy: 0.8095 - val_loss: 0.4957 - val_accuracy: 0.7775\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 80s 320ms/step - loss: 0.3989 - accuracy: 0.8200 - val_loss: 0.4794 - val_accuracy: 0.7830\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 80s 319ms/step - loss: 0.3930 - accuracy: 0.8236 - val_loss: 0.5023 - val_accuracy: 0.7815\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 80s 321ms/step - loss: 0.3846 - accuracy: 0.8231 - val_loss: 0.4654 - val_accuracy: 0.7965\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 81s 324ms/step - loss: 0.3732 - accuracy: 0.8313 - val_loss: 0.5300 - val_accuracy: 0.7655\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 81s 324ms/step - loss: 0.3597 - accuracy: 0.8391 - val_loss: 0.4771 - val_accuracy: 0.7985\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 80s 320ms/step - loss: 0.3392 - accuracy: 0.8480 - val_loss: 0.4810 - val_accuracy: 0.7885\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 80s 319ms/step - loss: 0.3415 - accuracy: 0.8480 - val_loss: 0.5394 - val_accuracy: 0.7800\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 80s 321ms/step - loss: 0.3201 - accuracy: 0.8620 - val_loss: 0.4866 - val_accuracy: 0.7985\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 81s 324ms/step - loss: 0.3238 - accuracy: 0.8593 - val_loss: 0.4933 - val_accuracy: 0.7910\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 79s 318ms/step - loss: 0.3125 - accuracy: 0.8645 - val_loss: 0.4933 - val_accuracy: 0.8025\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 79s 318ms/step - loss: 0.2923 - accuracy: 0.8670 - val_loss: 0.4977 - val_accuracy: 0.8020\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 80s 321ms/step - loss: 0.2851 - accuracy: 0.8777 - val_loss: 0.5887 - val_accuracy: 0.7745\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 79s 317ms/step - loss: 0.2920 - accuracy: 0.8775 - val_loss: 0.5189 - val_accuracy: 0.7965\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 80s 319ms/step - loss: 0.2657 - accuracy: 0.8865 - val_loss: 0.5086 - val_accuracy: 0.8110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efe364b2590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiLhdnj2aj0Q"
      },
      "source": [
        "## Part-4 Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JgSsOwRS7h4"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/drive/MyDrive/cnn dataset/dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "result = cnn.predict(test_image/255.0)\n",
        "training_set.class_indices\n",
        "if result[0][0] > 0.5:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qc4kSA8b-DB",
        "outputId": "c655cb15-0fa2-4bc4-9034-a3d489eaa6ae"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OUY7IS0gpek"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}